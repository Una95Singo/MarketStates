{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Project: “Potts Model Clustering”\n",
    "Author : Lionel Yelibi, 2019, University of Cape Town.\n",
    "Copyright SPC, 2019, 2020\n",
    "Potts Model Clustering.\n",
    "Agglomerative Fast Super-Paramagnetic Clustering\n",
    "See pre-print: https://arxiv.org/abs/1908.00951\n",
    "GNU GPL\n",
    "This file is part of SPC\n",
    "SPC is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "SPC is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>.'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from numba import jit\n",
    "\n",
    "''' the cluster function:\n",
    "    compute the likelihood which occurs when two objects are clustered or\n",
    "    the object own likelihood. By design if a cluster only containts one object\n",
    "    its likelihood will be 0'''\n",
    "@jit(nopython=True)\n",
    "def clus_lc(gij,gii,gjj, ns=2):\n",
    "    ''' variables description:\n",
    "        ns is the size of the cluster.\n",
    "        cs is the intracluster correlation.\n",
    "        gij, gii, and gjj respectively are\n",
    "        correlations relating to interactions between objects i and j, and\n",
    "        their respective self-correlations. self-correlations (i.e. gii, gjj)\n",
    "        are 1 for individual objects and >1 for clusters'''\n",
    "        \n",
    "    if ns==1:\n",
    "        return 0\n",
    "    ''' intracluster correlation'''   \n",
    "    cs = 2*gij+gii+gjj\n",
    "    ''' relatively low cs means noisy and suboptimal clusters.\n",
    "    The coupling parameter gs (see paper) isn't not defined'''\n",
    "    if cs<=ns:\n",
    "        return 0\n",
    "    return 0.5*( np.log(ns/cs) +  (ns - 1)*np.log( (ns**2 - ns) / ( ns**2 - cs) )  )\n",
    "\n",
    "''' aspc only requires a correlation matrix as input:\n",
    "    here we convert the correlation to a dictionary for convenience. adding new\n",
    "    entries in a dict() is much faster than editing a numpy matrix'''\n",
    "def agglo_spc(G, cn= None):\n",
    "    N = len(G)\n",
    "    gdic = { i : { j : G[i,j] for j in range(N)} for i in range(N)}\n",
    "    del G\n",
    "    \n",
    "    ''' tracker is dictionary which stores the objects member of the same clusters.\n",
    "        the data is stored as strings: i.e. cluster 1234 contains objects 210 & 890\n",
    "        which results in tracker['1234'] == '210_890' '''\n",
    "    tracker = { i:str(i) for i in range(N) }\n",
    "    \n",
    "    ''' the cluster size ns is stored in the ns_ array'''\n",
    "    ns_ = [1]*N\n",
    "    \n",
    "    ''' Create a list of object indices 'other_keys': at every iteration one object\n",
    "     is clustered and removed from the list. It is also removed if no suitable\n",
    "     optimal destination is found.'''\n",
    "    other_keys = list(range(N))\n",
    "    \n",
    "    ''' the operation stocks once there is only one object left to cluster as we\n",
    "    need two objects at the very least.'''\n",
    "    while len(tracker) != cn:\n",
    "        \n",
    "        ''' a random initialization:\n",
    "            pick a object 'node' at random to start clustering,\n",
    "            this might have a consequence on the final result depending on the data.\n",
    "            then loop through the other objects using 'nbor' and costs to store\n",
    "            the likelihood resulting from clustering 'node' to the objects in\n",
    "            'nbor'.\n",
    "            indices: stores the indices which are combinations of node and others.\n",
    "            costs: stores the cost which compute the difference between the likelihood\n",
    "            of the resulting cluster and the sum of the two individual objects forming\n",
    "            the result cluster.\n",
    "            '''\n",
    "        ''' the routine uses other_keys and removes elements everytime they are clustered\n",
    "        or can't be clustered anymore. If a cluster number is not provided the routine\n",
    "        stops there. If one is then it continues by looking at the elements in the\n",
    "        optimal cluster solution (tracker) and continues merging until the preset\n",
    "        number of clusters is met'''\n",
    "        if len(other_keys)>1:\n",
    "            node = np.random.choice(other_keys)\n",
    "        else:\n",
    "            node = np.random.choice(list(tracker.keys()))\n",
    "        nbor = list(tracker.keys())\n",
    "        nbor.remove(node)\n",
    "        costs = np.zeros(len(nbor))\n",
    "        indices = [(node,key) for key in nbor]\n",
    "        node_lc = clus_lc(0,gdic[node][node],0,ns = ns_[node])\n",
    "        k=0\n",
    "        for i,j in indices:\n",
    "            costs[k] = clus_lc(gdic[i][j],gdic[i][i],gdic[j][j],ns=ns_[i]+ns_[j]) - (node_lc+clus_lc(0,gdic[j][j],0,ns = ns_[j]))\n",
    "            k+=1\n",
    "            \n",
    "        ''' find the optimal cost which will be the object clustered with node'''\n",
    "        next_merge = np.argmax(costs)\n",
    "        \n",
    "        \n",
    "        ''' stopping conditions '''\n",
    "        if costs[next_merge]<=0:\n",
    "            if len(other_keys)>1:\n",
    "                ''' if no cost is positive then this node cannot be clustered further\n",
    "                and must be removed from the list'''\n",
    "                other_keys.remove(node)\n",
    "                continue\n",
    "            elif not cn:\n",
    "                ''' if no cluster number is provided then the routine has completed\n",
    "                and tracker is the final solution'''\n",
    "                cn = len(tracker)\n",
    "                continue\n",
    "            elif cn:\n",
    "                ''' if a cluster number is provided the routine continues and keeps\n",
    "                merging'''\n",
    "                pass\n",
    "        ''' on the other hand, the largest positive cost is the designated\n",
    "        object 'label_b' clustered to node which here is stored as 'label_a'.\n",
    "        new clusters 'new_label' take values superior to N.\n",
    "        tracker, as previously explained, stores joined strings of the clusters\n",
    "        contents'''\n",
    "        \n",
    "        label_a = node\n",
    "        label_b = indices[next_merge][1]\n",
    "        new_label = len(ns_)    \n",
    "        \n",
    "        ''' removes merged elements and update others with the new cluster.\n",
    "        only do it when a positive cost is found.'''\n",
    "        if costs[next_merge]>0:\n",
    "            other_keys = list(tracker.keys())\n",
    "            other_keys.remove(label_a)\n",
    "            other_keys.remove(label_b)\n",
    "            other_keys.append(new_label)\n",
    "    \n",
    "        \n",
    "        ''' Once a cluster is formed, the correlation matrix gdic and tracker need to\n",
    "        be updated with the new cluster and the cluster size must be updated with ns_'''\n",
    "        nbor.remove(label_b)\n",
    "        tracker[new_label]=tracker[label_a]+'_'+tracker[label_b]\n",
    "        gdic[new_label]={}\n",
    "        gdic[new_label][new_label] = 2*gdic[label_a][label_b] + gdic[label_a][label_a] + gdic[label_b][label_b]\n",
    "        ns_.append(ns_[label_a] + ns_[label_b])\n",
    "    \n",
    "        for key in nbor:\n",
    "            gdic[new_label][key] = gdic[label_a][key] + gdic[label_b][key]\n",
    "            gdic[key][new_label] = gdic[label_a][key] + gdic[label_b][key]\n",
    "    \n",
    "        del tracker[label_a]\n",
    "        del tracker[label_b]\n",
    "    \n",
    "    \n",
    "    ''' create the final clustering array:\n",
    "        tracker contains the cluster memberships but as a dictionary\n",
    "        we create a numpy array where stocked are labeled with the same number\n",
    "        if they belong to the same cluster, and 0 if unclustered'''\n",
    "        \n",
    "    solution = np.zeros(len(data),dtype=int)\n",
    "    k=1\n",
    "    for cluster in tracker.keys():\n",
    "        cluster_members = np.int_(tracker[cluster].split('_'))\n",
    "        solution[cluster_members] = k\n",
    "        k+=1\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  it is possible to provide a cluster number, if none is provide\\nset cn = None'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5000\n",
    "data,_ = make_blobs(n_samples=N,n_features=400,shuffle=False,centers=10,random_state=0)\n",
    "'''  it is possible to provide a cluster number, if none is provide\n",
    "set cn = None'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_ = np.corrcoef(np.transpose(data))\n",
    "type(G_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.41778804, -0.16006788, ...,  0.26941005,\n",
       "         0.41408688,  0.3422683 ],\n",
       "       [-0.41778804,  1.        ,  0.22891518, ..., -0.05192743,\n",
       "        -0.0115642 , -0.58555303],\n",
       "       [-0.16006788,  0.22891518,  1.        , ...,  0.39231744,\n",
       "         0.31302713, -0.65464202],\n",
       "       ...,\n",
       "       [ 0.26941005, -0.05192743,  0.39231744, ...,  1.        ,\n",
       "         0.83156428, -0.08729482],\n",
       "       [ 0.41408688, -0.0115642 ,  0.31302713, ...,  0.83156428,\n",
       "         1.        ,  0.06251249],\n",
       "       [ 0.3422683 , -0.58555303, -0.65464202, ..., -0.08729482,\n",
       "         0.06251249,  1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol  = agglo_spc(np.corrcoef(np.transpose(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 19, 69, ...,  0,  0,  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"../R/Python/SimData/1_1.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ = pd.DataFrame.corr(Data).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol  = agglo_spc(g_, cn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9471676883805062"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(TrueStates.values.flatten(), sol[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1,\n",
       "       2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2,\n",
       "       1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2,\n",
       "       1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "       2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(TrueStates.values.flatten(), sol[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueStates = pd.read_csv(\"../data/PythonNohTestData3.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5370254751773159"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(TrueStates.values.flatten(), sol[0:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum(b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
